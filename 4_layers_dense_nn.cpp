#include <stdlib.h>
#include <math.h>
#include <stdio.h>

#include "nn_function.h"


# define inputLength 19

// This is a dense neural network with 4 layers, weight updated by SGD
# define layer1_neuronNum 61    // ⬇
# define layer2_neuronNum 16    // ⬇
# define layer3_neuronNum 12    // ⬇
# define layer4_neuronNum 1     // ⬇

// 把保存的权重复制到这下面

static double savedWeightTensor_1[layer1_neuronNum][inputLength + 1] =
{
{ -0.945076, 0.215717,-0.976682, 0.382603,-0.471381, 0.485755, 0.483074, 0.212940, 0.364147,-0.975523, 0.712967,-0.498849,-0.481517,-0.408778, 0.019803,-0.646640, 0.215983,-0.579446, 1.657587,-0.698550 },
{ -0.644575, 0.291409, 0.065773, 0.181756,-0.441560,-0.702979,-0.766958, 0.518514,-0.489738, 0.928661, 0.073786,-0.886585,-0.304705,-0.601313, 0.246827,-0.030306, 0.024821,-0.652765,-0.464897, 0.029519 },
{  0.465972, 0.088860, 0.163899, 0.024116, 0.393813,-0.146997, 0.265268, 0.314477, 0.010672, 0.396105, 0.280959,-0.134795, 0.428911, 1.083516, 0.317475, 0.031226, 0.207842, 0.262583, 0.787746,-1.378496 },
{ -0.566011, 0.402850, 0.227302,-0.708166, 0.215838,-0.782645,-0.590096, 0.680048,-0.061161,-0.367303, 0.395638,-0.241660,-0.480657, 0.916585, 0.137914,-0.457801, 0.237175,-0.222961, 0.246767,-0.871975 },
{  0.182052,-0.563513,-0.116150,-0.049584,-0.638121,-0.293988,-0.904802, 0.031517,-0.147550,-0.195685,-0.088364,-0.901971, 0.114091,-0.482377, 0.000154,-0.208821,-0.760668,-0.164515, 0.878299, 0.464304 },
{  0.068892,-1.504353,-0.490027, 0.678127,-0.517578, 0.630498,-0.145953, 0.918802, 0.028440,-1.518664,-0.356263, 0.779556,-0.590868, 0.366615,-0.362553, 0.567393,-0.004323,-1.568931,-1.960950,-0.025094 },
{ -0.121701, 0.199989,-0.139705,-0.413190, 0.064453, 0.059202,-0.098151,-1.281458,-0.427009, 1.245675,-0.068995,-0.040620,-0.254144, 0.969860,-0.267993,-0.558058,-0.177582,-0.403778,-1.980342, 0.407240 },
{  0.706619,-0.323058, 0.455335, 0.806847, 0.627055,-0.739407,-0.119523,-0.375018,-0.372946,-0.067969, 0.163791,-0.402236,-0.856593, 0.156436,-0.569339, 0.619967,-0.149993, 0.103821, 0.703565,-0.686155 },
{  0.339788,-0.317670, 0.410124,-0.240146,-1.040810, 0.519301, 0.136325,-0.017265,-0.717904, 0.273508, 0.324482,-0.384453,-0.836400, 0.475837, 0.456089,-0.705422, 0.648411,-0.662189,-0.102203,-0.751444 },
{ -0.796814, 0.419323, 0.241224,-0.008788, 0.603221,-0.391248,-0.671579, 0.631302, 0.901270,-1.021103, 0.371834,-0.112917,-0.588692, 0.504787, 0.215339, 0.242291, 0.379616,-0.663602, 1.470573,-0.931223 },
{ -0.365620,-0.251523, 0.069136,-0.159897, 0.054871,-0.812433,-0.249422,-1.156917, 0.010730, 0.089089,-0.949106, 0.075424,-0.792465,-0.039643,-0.242694,-0.215833, 0.305198,-0.325017, 0.111370, 1.088371 },
{ -0.295655, 0.475262,-0.534986, 0.494609, 0.750513,-0.962928,-0.310030, 0.299896, 0.286011,-1.282126,-0.610118, 0.265803, 0.748060,-1.006245,-0.837434, 0.161462,-0.515854, 0.596057,-0.877395,-0.439412 },
{ -0.765037, 0.987318, 0.362212,-0.287022,-0.017677,-0.067058, 0.593130,-0.382588,-0.260142, 0.247718, 0.009208, 0.389405,-0.489464,-0.111748,-0.383458, 0.217316,-0.111573,-0.321556,-0.579076,-0.507518 },
{  0.414783,-0.519146,-0.060801, 0.303318, 0.387590,-0.078778,-0.337118, 0.888650, 0.002205, 0.456290,-0.258107, 0.537794,-0.017312,-0.434532,-0.145399, 0.311876, 0.863930,-0.544524, 0.307986,-1.261058 },
{ -0.226343, 0.110785,-0.494806, 0.206994,-0.106613,-1.028052,-0.177876,-0.320190, 0.019145, 0.433508,-0.418458, 0.573919,-0.164589, 0.149792,-0.086598,-0.776259, 0.301648, 0.744757,-1.433934, 0.323137 },
{ -1.277149,-0.084448,-0.072950,-1.257337,-0.501713, 0.117616,-0.039246,-0.367146,-0.026997,-0.103737,-0.283425,-0.125466, 0.320067,-0.678053,-1.080001, 0.134120, 0.003354,-0.908725, 0.259375, 0.738979 },
{ -0.979415,-0.098094,-0.152323,-0.385060, 0.628674,-0.839091,-0.335496,-0.179299, 0.102001,-0.720442,-1.205930, 0.125992,-0.861633, 0.033416, 0.035825,-0.039946, 0.613716,-0.636178, 0.246345,-0.102340 },
{ -0.513765,-0.410963,-0.020759, 0.419290,-0.136969,-0.492100,-0.223952, 1.113966,-0.186270,-1.609317,-0.110113, 0.685687, 0.624908,-0.541843, 0.458321, 0.311876,-0.068069, 0.116236,-0.695962,-0.408251 },
{  0.486989,-0.491217, 0.266506,-0.348120,-0.409263, 0.645809,-0.119169,-0.404441, 0.745232,-1.224668,-0.840529, 0.395695, 0.099578,-0.599098, 0.270114, 0.078672,-0.652614, 0.652005, 1.712605,-1.110778 },
{  0.571492,-0.538902, 0.977305,-0.967770, 0.868507,-0.417984,-0.931969, 0.612606,-0.696010, 0.553342,-1.140317, 0.791264, 0.003896,-0.538283, 0.396819,-0.593034,-0.313432,-0.155372, 1.184113,-1.489982 },
{ -1.006156, 0.740792,-0.908644, 0.853155,-0.774972, 0.844118,-0.513634, 0.504778,-1.337503, 0.109169,-0.547428, 0.335007, 1.478601,-1.059751, 1.560300,-1.050383, 1.329008,-1.195406,-1.858723,-0.847545 },
{  0.487238,-0.380233, 0.033927,-0.094311,-0.398363,-0.228754, 0.021601,-0.441903,-0.274237, 0.702142,-0.503568,-0.319269,-0.376776,-0.786466,-0.204137,-0.143265, 0.054180, 0.420998,-0.454090, 0.220422 },
{ -0.514814,-1.252603, 0.268406, 0.533715,-0.023442, 0.515488,-0.621878, 0.246106,-0.656071,-0.365451, 0.184763,-0.451724, 0.419564,-0.219683, 0.154544, 0.036804,-0.357473, 0.420357,-0.452466,-0.148189 },
{ -0.629218, 0.996822, 0.032372,-0.471436,-0.242181, 0.035339, 0.423132,-0.773815,-0.661532, 0.909011, 0.280282,-0.327988,-0.055272,-0.644838, 0.293760,-0.742414,-0.490213, 1.097681,-1.285327,-1.271117 },
{ -0.162678,-0.158482,-0.005974, 0.788510,-0.325656, 0.101043, 0.115843, 0.249918, 0.710713, 0.832411, 0.165077, 0.289292, 0.172222, 0.142906, 0.436046,-0.078490,-0.047537, 0.293045,-0.317088,-1.145769 },
{ -0.707185, 0.166964,-0.321010, 0.196188,-0.032167,-0.759495, 0.517595,-0.618710, 0.126949, 0.023058,-0.422995, 0.275974,-0.327704, 0.798747, 0.053108,-0.119243,-0.433499,-0.513352,-0.545013,-0.065675 },
{ -0.721262, 0.589367, 1.428707,-0.606190,-0.877216, 0.628179,-0.720348, 0.674080, 0.828396,-1.240414,-0.846034, 0.720897,-0.637057, 0.807929, 1.276900,-1.063126,-0.652308, 0.747339,-2.264891,-0.685622 },
{ -0.072323,-0.574763,-0.571641, 0.447335,-0.021424,-0.566532, 0.177178,-0.216045,-0.665268, 0.449946, 0.097759,-0.189037, 0.579244,-0.482805,-0.893047, 0.727269, 0.464308,-0.241300,-0.835164, 0.289120 },
{ -0.208028, 0.089354, 0.467228,-0.439825,-0.158555, 0.154778, 0.216749,-0.493911,-0.207645, 0.652352, 0.436140,-0.481999,-0.570230, 0.309864, 0.149509,-0.487709,-0.300351, 0.467156,-0.911488,-0.784734 },
{ -0.549319,-1.063134,-0.096962, 0.783932,-0.227578,-0.806078,-0.231970, 0.672673, 0.040444, 0.280127, 0.059013,-0.292109, 0.008393, 0.176924,-0.219374,-1.353528,-0.812498, 0.231834,-0.739140, 0.708533 },
{ -0.117238,-0.032060, 0.350949, 0.584888,-0.171518, 0.034076, 0.307565,-0.598576,-0.811612, 0.210462, 0.369094,-0.812522,-0.504808, 0.183616,-0.603437, 0.033676,-0.072869,-0.190015, 0.344626,-0.168697 },
{ -0.619851,-0.029946, 0.308642,-0.264696, 0.314016, 0.246804,-0.079388,-0.059359,-0.488263, 0.225373, 0.467056,-0.551703,-0.063783, 0.167678, 0.590595,-0.462167, 0.412363, 0.322635, 0.639279,-0.872442 },
{ -0.061316, 0.255297,-0.437065,-1.462961,-1.425060, 0.435720,-0.681854, 0.551631,-0.020416, 0.278387,-0.023385,-0.634432,-0.000189,-1.274423,-0.611849, 0.875676, 0.047155,-0.036419,-0.804080, 0.932879 },
{ -0.335693, 1.084211, 0.056511,-1.257410, 0.311510, 0.478417,-0.551231, 0.247447, 0.063834,-0.632273,-0.317099, 0.209900,-0.330949,-0.198678,-0.061458,-0.396099,-0.309095,-0.185484,-0.931824, 0.538065 },
{ -0.978718, 0.680062,-1.161756, 0.967336,-1.011119, 0.985645, 0.489018,-1.123519, 0.220842,-1.278247, 0.417049,-0.681104, 0.778004,-0.783406, 0.485570,-0.147313, 0.337529,-0.652578, 0.421457,-1.517383 },
{  0.487158,-0.494086, 0.218356,-0.540633,-1.128703, 0.445586, 0.774512,-0.283405,-0.182434,-0.255748,-1.017403, 0.845266, 0.534380,-0.375821, 0.202244,-0.842322,-1.202944, 0.459690, 0.447802,-0.849302 },
{  0.315555, 0.272526,-0.284556, 0.888598, 0.209013, 0.125960, 0.509570,-0.667214, 0.119980, 0.438148, 0.384352,-0.718689, 0.136126,-0.086197,-0.391451, 0.806598,-0.040108, 0.276952,-0.125042,-1.013455 },
{  0.236377,-0.263562, 0.574096,-0.778461, 0.094202, 0.094208, 0.785805,-0.740154, 0.466230,-1.149296, 0.474253,-0.811830,-1.297714, 0.739327,-0.964534, 0.778608,-1.281422, 0.348989, 0.611463,-1.383162 },
{ -0.175269, 1.022247,-0.131232,-0.768418, 0.308716,-0.263776, 0.224027,-0.065881,-0.671926, 0.090441,-0.264718, 0.058442, 0.550511, 0.022775, 0.307386,-0.296059, 0.254766, 0.144425, 0.592001,-0.944804 },
{  0.802369,-0.171789,-1.036261, 0.722434, 0.803470,-0.100345,-0.254996, 0.005203,-0.393373,-0.759801,-0.200494,-0.194186,-0.752750,-0.586702,-0.086308,-0.738837,-0.368321,-0.338212,-0.266139, 0.709258 },
{  0.194378,-0.210734, 0.097012,-0.495624,-0.120742, 0.282265,-0.419678, 0.548300,-0.253643, 0.561825, 0.063560,-1.017809,-0.391546,-0.597588,-0.223152, 0.200511, 0.067107, 0.271043,-0.823343,-0.066956 },
{ -0.000064,-0.269485, 0.089895,-0.381520, 0.852507,-0.665910,-0.204053,-0.093605,-0.923515, 0.280287, 0.401643,-0.435948,-0.979810, 0.092697,-0.355312,-0.354848,-0.169223,-0.062387, 1.688712,-0.030780 },
{  0.454580,-0.668884, 0.226432,-0.490643,-0.899964, 0.908323, 0.361637,-0.673462,-0.572038, 0.438251, 0.630980,-0.641015,-0.965937, 0.695913, 0.080292,-0.631158, 0.229053,-0.361756, 0.098684,-0.819715 },
{ -0.214996,-0.290414,-0.081085,-0.439933,-0.343411, 0.908210,-0.241143, 0.708343,-0.368256, 0.800680,-0.668616, 0.640468, 0.249711,-0.313944, 0.518645,-0.950516, 0.550515,-0.724659,-1.624445,-0.476418 },
{  1.239711,-1.175649,-0.480849, 0.647221,-0.616496, 0.719759, 1.408511,-1.268296,-1.388730, 0.598636,-0.554054, 0.763098, 0.922483,-0.854872,-0.997270, 0.571918,-0.913716, 0.928357,-1.941973,-0.788979 },
{  0.440749,-0.693537,-0.847457, 0.251745, 0.039832,-0.937121, 0.027138,-0.196000, 0.056654, 0.075423, 0.040352, 0.225728,-1.124065,-0.016267, 0.028297,-0.651085,-0.395122, 0.305864,-0.323207, 0.426135 },
{  0.696377,-0.898301, 1.199176,-1.041621, 0.924177,-1.061586,-0.678011, 0.473227,-1.193485,-0.044599,-0.485364, 0.142099,-0.439485, 0.928839,-0.678982, 1.013833,-0.898025, 0.561167,-1.156847,-0.639360 },
{  0.393049,-0.240347,-0.879892,-0.153686, 0.488421,-0.271735, 0.828704,-0.378485, 0.511603,-0.555291, 0.670841,-0.285579,-0.372108, 0.527419,-0.933766, 0.196397,-0.301455, 0.354336, 0.799642,-1.016109 },
{ -0.543977, 0.464718,-0.697431, 0.384544,-0.740630, 0.643631, 0.946750,-0.939998, 1.266730,-1.260339, 1.254449,-1.200242,-0.629906, 0.900760,-0.962090, 0.389762,-0.378342, 0.981311,-1.420158,-1.399342 },
{ -0.783861, 0.813312,-0.638186, 0.515452, 1.281909,-0.891905,-0.741717, 0.681345,-1.373090, 0.684937, 1.490501,-1.342237,-0.995277, 0.714179,-0.924622, 0.562215, 1.222934,-1.129510,-2.122572,-0.540811 },
{  0.360804,-0.521212,-0.829298, 0.365153, 0.482899,-0.557032, 0.300131,-0.344114,-0.455487, 0.531489, 0.566381,-0.242532, 0.652775,-0.565931,-0.673317, 0.379152, 0.469733,-0.701343, 2.050015,-1.484101 },
{  0.034726, 0.177041, 0.231411,-0.213712, 0.914751,-0.479494,-0.359002, 0.553379, 0.619425,-0.202862,-0.383870, 0.221846,-0.323720,-0.111928,-0.440221, 0.277528,-0.604862, 1.072379,-0.139390,-0.405009 },
{  0.779214,-0.587339,-0.692225, 0.255620,-0.488287, 0.447197,-0.676596, 0.331723, 0.876900,-1.141863,-0.597218, 0.474211,-0.699696, 0.520727,-0.529706, 0.649720, 1.034880,-0.633276,-0.918838,-0.956379 },
{ -0.030286, 1.295004, 0.067577, 0.318993, 0.060538,-0.903639, 0.025534,-0.162701,-0.399348,-0.852770,-0.287198, 0.218912,-0.241345,-0.577203,-0.165062, 0.812475,-0.282186,-0.120118,-1.709406, 0.197900 },
{  0.481078,-0.801215,-0.313405,-0.771841,-0.109094,-0.624642,-0.305358, 1.126085,-0.238592, 0.404857,-0.176750, 0.045891,-0.031767,-0.279118,-0.122143,-0.040701,-0.167687, 0.293816,-0.730423, 0.370233 },
{ -0.066697,-0.349081,-0.012372,-0.462172,-0.732170, 0.001257, 0.286113,-0.308367,-0.849607, 0.158196,-0.350015,-0.209346,-0.174356, 0.513958, 0.943765,-0.209278, 0.118102,-0.355916, 0.245554,-0.402565 },
{ -0.965716, 0.885938, 0.484376,-0.219086, 0.388642,-0.694553, 0.619976,-0.541739,-0.719493, 0.689626, 0.480459,-0.787211, 0.360123,-0.724245, 0.389851,-0.565097,-1.110573, 0.771761, 0.278411,-1.550889 },
{ -0.764634, 0.202955,-0.191404,-0.163134, 0.121626,-0.153219,-0.077179,-0.566810, 0.096395, 0.371984,-0.170460, 0.847487,-0.068837,-0.289349,-0.068431, 0.670550,-0.254704,-0.774493,-0.785319, 0.584436 },
{  0.971441,-0.578369, 1.116114,-0.242284, 0.771722,-0.606492,-0.603929, 0.184886,-1.040434, 0.726902,-0.658899, 0.290743,-0.774883, 0.646056,-0.220803, 0.079985,-0.567026, 0.580354,-1.285751,-0.244496 },
{ -0.395940,-0.240185,-0.214254, 0.336909,-0.036589, 0.173764,-0.037673,-0.582149,-0.321955, 0.756935, 0.104869,-0.545191,-0.268398, 0.369117,-0.328409, 0.333065, 0.338115,-0.583756,-0.886180,-0.593831 },
{ -0.187159,-0.817531,-0.292493, 0.084996,-0.042816, 1.182652,-0.424819, 0.290010, 0.134830, 0.249761,-0.248525,-1.550198,-0.260208,-0.750956,-0.391799,-0.271926,-0.446194, 0.644185,-0.964807, 0.664408 },
};
static double savedWeightTensor_2[layer2_neuronNum][layer1_neuronNum + 1] =
{
{  0.913449, 0.161837,-0.366056,-0.350256, 0.023404,-0.543399, 0.569621, 0.297418,-0.446606,-0.241581, 0.018118,-0.022911,-0.002809,-0.254371, 0.150589,-0.077048,-0.107322,-0.117194,-0.110679,-0.444343,-0.045944,-0.029059,-0.040393,-0.065303, 0.470767, 0.405653, 0.762035, 0.348366, 0.096621,-0.370803,-0.134647,-0.330484, 0.158661, 0.262490, 0.452819,-0.320026, 0.033689,-0.598172,-0.189854,-0.091019,-0.072614,-0.202138,-0.379149, 0.574881,-0.482013,-0.090411,-0.313838,-0.520855, 0.637472, 0.199002, 0.579538,-0.312098,-0.194524, 0.109752, 0.070854,-0.473620,-0.383014,-0.444128,-0.244559, 0.146821, 0.233889,-0.192454 },
{ -0.223594,-0.295779,-0.398323, 0.127997, 0.043136,-0.529194,-0.297292,-0.290213, 0.056475,-0.372641, 0.069544,-0.385528, 0.181963, 0.561172, 0.383813,-0.245997,-0.191020, 0.737845,-0.220942, 0.002083,-0.077035,-0.322482, 0.166706,-0.190496, 0.044163, 0.021476,-0.268864,-0.345132,-0.231697, 0.390191,-0.316235,-0.150137, 0.270492,-0.200840, 0.275739,-0.179473,-0.246475, 0.257656,-0.036452, 0.230753,-0.171408,-0.309669,-0.235156, 0.679619, 0.022046, 0.018526, 0.025075, 0.241374,-0.208760,-0.198634,-0.236212,-0.111872,-0.103198,-0.344133, 0.750248, 0.125120,-0.249661,-0.255505, 0.008618,-0.250428, 0.170025,-0.628188 },
{  0.346440,-0.053338,-0.081998,-0.034602, 0.061144,-0.456403,-0.685762,-0.178577, 0.053411,-0.069570,-0.047125,-0.066927,-0.075359,-0.222830,-0.658899, 0.403068, 0.074406,-0.306442,-0.367247, 0.607089, 0.879866,-0.263474,-0.403484,-0.605769,-0.070604,-0.135318, 0.063808, 0.010618,-0.250800,-0.347708,-0.241492, 0.249759,-0.234156,-0.882788, 0.488030,-0.488040, 0.200871, 0.619985,-0.071960, 0.287111,-0.288968,-0.317520, 0.102361,-0.259534,-0.558368,-0.284455, 0.279374, 0.381264, 0.999664,-0.309721,-0.566513,-0.156722,-0.024405,-0.615667,-0.121557,-0.102828, 0.336327,-0.298018,-0.531157,-0.237072,-0.423496,-1.187024 },
{ -0.559730, 0.407530,-0.023505, 0.052818, 0.304782, 0.299161, 0.831409, 0.260462,-0.512245, 0.340375, 0.004233,-0.490363,-0.035892,-0.603096, 1.062875, 0.038054, 0.828107,-0.004284, 0.269337,-0.997318, 0.415933, 0.047240, 0.065301,-0.784108,-0.154444, 0.304724,-1.335835, 0.389701,-0.206270, 0.248495,-0.161118,-0.199465, 0.333098,-0.011590,-0.322173, 1.025237,-0.717910, 1.060816,-0.602608, 0.230607, 0.503752, 0.382278,-1.078772,-0.680395,-0.983117, 0.231312,-0.088137, 0.680161,-0.852867, 0.483484,-1.354781, 0.214589,-0.372277, 0.342452, 0.326002, 0.211775,-0.686820, 0.396519,-0.291246, 0.115394,-0.137194, 0.416352 },
{ -0.619873, 0.556761,-1.031735,-1.158753,-0.303469, 1.059651, 0.003319,-0.491529,-0.433156, 0.177603,-0.251187, 0.921816,-0.002856, 0.189706,-0.209339,-0.175252,-0.402042,-0.006056,-0.758667,-0.715446, 1.086953, 0.131603,-0.366793,-0.478541, 0.415432,-0.494085,-0.370103,-0.495026,-0.518368,-0.046242,-0.519239,-0.544782, 0.254550, 0.431735,-1.637887,-0.724865,-0.061281,-0.112216,-0.365457, 1.000833, 0.130992,-0.519064,-0.096090, 0.955245, 1.061839,-0.116914,-0.518252,-0.071145, 0.894868,-0.120950,-1.009837, 0.445969, 1.097328, 0.301845,-0.498894,-0.758808,-0.168733, 0.323548,-1.016338,-0.672781, 0.427831,-0.596348 },
{  0.200573,-0.121796,-0.105402,-0.201440,-0.407406,-1.062743, 0.105344,-0.023817, 0.069056, 0.889269,-0.762637, 0.938736,-0.214839, 0.864108,-0.550544,-1.218481,-0.670103, 0.277426, 0.571852,-0.193558, 0.123074,-0.564816,-0.545151, 1.014965, 0.456395,-0.195017, 0.658371,-0.306402, 0.926265,-0.747224,-0.061906, 0.340369,-1.026392,-0.151121, 0.006612, 0.450047, 0.158345, 0.342772, 0.212773, 0.089365,-0.418766,-0.515781, 0.843671,-0.205532, 0.965940,-1.038350, 0.107324, 0.301798, 0.004659, 1.195964, 0.625599, 0.437390, 0.996569,-0.178443,-0.691197, 0.229938, 0.236960,-0.406186,-0.460002, 0.396583,-0.939916,-0.603048 },
{  0.007728, 0.148135,-0.526532, 0.083870,-0.246200, 0.421218,-0.021950,-0.342093, 0.018178,-0.149621, 0.228971, 1.115067,-0.444238,-0.156380, 0.244977,-0.040117,-0.154099, 0.589789,-0.398477,-0.450280,-0.675177, 0.131500,-0.247389,-0.232366,-0.210494, 0.056493, 0.461094, 0.194233,-0.208023, 0.191170,-0.388166,-0.479144,-0.046109,-0.504029,-0.085789,-0.600064,-0.331630,-1.461693,-0.784107,-0.331281, 0.139876,-0.379655,-0.312228,-0.663861, 0.192969,-0.014800, 1.244521,-0.535684,-0.967104,-0.667350,-0.566326, 0.264523, 0.729533, 0.065058, 0.496249,-0.240723,-0.523637, 0.442683, 0.806186,-0.074794,-0.019132,-0.465179 },
{ -0.364098,-0.324827,-0.050779,-0.231790,-0.218183,-0.215557,-0.347054, 0.105664,-0.101886,-1.063624,-0.243925,-0.024866, 0.378667, 0.055951,-0.036845, 0.322134, 0.017197, 0.080291, 0.483910, 0.103851, 0.154658,-0.174202,-0.128088,-0.199562,-0.089456,-0.120518,-0.528471, 0.191967,-0.263483,-0.087623, 0.162122,-0.443570, 0.189068,-0.104111, 0.118785, 0.693354,-0.097586, 0.160006, 0.266891,-0.072748, 0.250162,-0.504104,-0.076867,-0.202976,-0.188307, 0.317863, 0.115198,-0.229348,-0.330466,-0.160750,-0.855393,-0.386257,-0.529291, 0.283339, 0.011868,-0.144271,-0.440958, 0.015138, 0.373731, 0.025862, 0.344747,-0.334457 },
{  0.591642,-0.264548,-0.093149,-0.049620,-0.222760,-0.541944,-1.150548, 0.365641,-0.574361,-0.336271,-0.276386,-0.099486,-0.062438, 0.547923,-0.086275,-0.070519,-0.458043,-0.522365,-0.619131, 1.059034, 0.650909,-0.415542,-0.282359,-0.518806, 0.480547,-0.099997,-0.572622, 0.410186,-0.411947,-0.674742,-0.289053,-0.333086,-0.633327,-0.025746, 0.449970,-0.092367, 0.485441, 0.897489,-0.319918,-0.795095,-0.591718,-0.363219,-0.662872, 0.345952,-0.506214,-0.177330, 1.119254, 0.426076, 1.442663,-0.506892,-0.398465, 0.278827,-0.211534, 0.250159, 0.407756,-0.067521, 0.057137, 0.191807, 0.464395, 0.274837,-0.429496,-1.770349 },
{  0.267643,-0.225671, 0.150084, 0.432121, 0.015406, 0.288535,-1.081879,-0.226078, 0.607424,-0.507882,-0.509920,-0.429469,-0.433985, 0.279528,-0.173104,-0.476105, 0.771882, 0.299649, 0.385995, 0.552655,-1.227970,-0.489744,-0.400253,-0.861857, 0.070650,-0.564206,-0.967855, 0.327192,-0.221806,-0.250425, 0.827639, 0.556130,-0.125237, 0.033423, 0.709563, 0.595247, 0.030307,-0.421397, 0.551518,-0.805124,-0.125196, 0.328164, 0.883332,-0.262030,-1.195557,-0.500158,-0.917404,-0.947892,-0.715553,-1.039387, 0.035444,-0.741441,-0.899333,-0.339931,-0.490253, 0.428899, 1.628898,-0.407497,-0.588088,-0.446923, 0.012610, 0.044824 },
{ -0.216222, 0.201952,-0.153539, 0.050807,-0.223882,-0.311193,-0.617532,-0.221009,-0.170063, 0.334028,-0.375074,-0.304451, 0.041015,-0.250351,-0.406818,-0.117351, 0.198059, 0.365677, 0.128581,-0.131168,-0.215994, 0.078488,-0.111708,-0.050590, 0.046939,-0.192380, 0.493592,-0.197476,-0.183562,-0.070614,-0.285978, 0.042530,-0.271170,-0.021639, 0.337945, 0.386913,-0.358896,-0.344077,-0.384392, 0.261460,-0.015177, 0.026335,-0.379676,-0.087053, 0.659418,-0.261567, 0.133282,-0.136824, 0.237794, 0.072948,-0.086030, 0.076317, 0.072755,-0.167477, 0.008108,-0.218246,-0.324707,-0.103914,-0.439847, 0.184821,-0.322486,-0.214102 },
{ -0.512694, 0.633871,-0.171491,-0.432776,-0.177779, 0.774761, 0.111568,-0.674083, 0.317648,-0.375938, 0.473133, 0.300041, 0.079450, 0.383477,-0.564145,-0.072650,-0.309950, 0.736583,-0.757611, 1.394630,-0.824594,-0.392409,-0.584909,-0.431986,-0.053695, 0.021759,-0.773541,-0.804570,-0.331416, 0.307710, 0.021730,-0.161705,-0.022369,-0.980706,-0.449827,-0.197312,-0.106743,-0.847592,-0.129111, 0.375436,-0.238707,-0.534910,-0.300970,-0.906580,-0.305780,-0.030381, 0.405238,-0.500651, 1.463933, 1.121707,-0.234134,-0.927519, 0.518446, 0.687220, 0.596037,-0.350749,-0.037730,-0.155967,-0.365924, 0.173643,-0.698251, 0.241041 },
{ -0.016971,-0.197152, 0.288473,-0.304346, 0.895278, 0.107904, 0.446467,-0.065645,-0.592042,-0.089431, 0.706871,-0.464056, 0.129155,-0.259262, 0.398914, 0.629167, 0.251442, 0.090880, 0.097818,-0.134700,-0.189765, 0.276091, 0.803809,-0.068827, 0.417864, 0.471879,-0.053547,-0.447431, 0.059159, 0.909424,-0.020607, 0.281468, 0.526769,-0.033681,-0.280092,-0.438067,-0.161995,-0.416287,-0.185218, 0.564059,-0.014793, 0.048710,-0.197457,-0.095970,-0.116215, 0.942394,-0.540498, 0.467464,-0.032272,-0.324880,-0.179556,-0.245658,-0.620383,-0.295834,-0.052637, 0.378395,-0.306651,-0.117180,-0.100947, 0.264091, 0.608195,-0.141837 },
{ -1.148144,-0.552611,-0.062346,-0.326701,-0.386211, 0.997858,-0.420830, 0.588269, 0.484540,-0.726909,-0.288104, 0.576376,-0.135101, 0.418534, 0.090471, 0.220618,-0.610429, 0.881890,-0.777516, 0.308747,-0.979226,-0.260855,-0.333805,-0.252502,-0.529967,-0.083479, 1.400147, 0.783814,-0.213677,-0.328450, 0.238458,-0.238727,-0.098222, 0.917016,-0.324934,-0.544278, 0.629697, 1.187423, 0.206336, 0.094435,-0.048595, 0.603382, 0.782142,-0.442113,-0.960300,-0.114379,-1.305060, 0.117645, 0.998951,-1.244513, 0.366469, 0.222919, 0.035884, 0.745868,-0.161023, 0.491235, 0.417467,-0.414751,-1.252909,-0.210951, 0.162286, 0.106409 },
{  0.631869,-0.147803,-0.718034,-0.126297, 0.445305, 0.319935, 0.420548,-0.646199,-0.425853, 0.293468, 0.637248,-0.457326, 0.924755,-0.390973,-0.148565, 1.208104, 0.585987,-0.075810,-0.031579, 0.031574,-0.490154, 0.245795, 0.397366,-0.384301,-0.363232, 0.535921,-0.138321,-0.695339, 0.283335, 0.192351, 0.264777,-0.253190, 1.017469,-0.179163,-0.131250,-0.287898,-0.726969,-0.249229,-0.110083, 0.334446, 0.863159, 0.342397,-0.047688,-0.628527,-0.113404, 0.008533,-0.180119, 0.157818, 0.248958, 0.284149, 0.282225,-0.479602,-0.065453, 0.611823, 0.372582, 0.410808,-0.463512, 0.697992,-0.355613, 0.189463, 0.058843, 0.653062 },
{  0.583144,-0.398220,-0.390423, 0.844373,-0.572193,-0.096123,-0.316374,-0.931243, 0.265651, 0.852914,-0.328139,-0.237593,-0.258118,-0.312442, 0.428473, 0.339830, 0.327294,-0.546229,-1.246141, 0.357287,-1.158054, 0.412500, 0.466895,-0.022311,-0.183163,-0.411380,-1.316649, 0.249469,-0.211158, 0.206342,-0.363141, 0.616683, 0.391771,-0.038056, 1.542794, 0.208066,-0.147209,-0.569825, 0.127746,-0.339864,-0.268927, 0.287410,-0.135762,-0.458010, 1.211460, 0.000047,-0.186482,-0.982037, 0.085425,-1.264423,-0.575976, 0.115807,-0.458365,-0.465373,-0.003334, 0.330906,-0.176757,-0.000867,-0.180963,-0.184946, 0.281279,-0.572569 },
};
static double savedWeightTensor_3[layer3_neuronNum][layer2_neuronNum + 1] =
{
{  0.064307, 0.266365, 0.401617,-0.505914,-0.243996, 0.656096,-0.355801,-0.019662, 0.447705,-0.327018, 0.085311,-0.124310,-0.846042,-0.310252,-0.758045,-0.199071, 1.668819 },
{  0.094008, 0.727260, 1.393067,-0.791365,-0.882824, 0.597860,-0.691239, 0.274665, 1.850574,-1.123706, 0.946171,-0.052163,-0.558198,-0.680997,-0.821241,-0.030985, 0.018706 },
{  0.013786,-0.210611, 0.365981,-0.334742,-0.452347,-0.091083, 0.324793,-0.136043,-0.199857,-0.329382, 0.001261, 0.002588, 0.054510, 0.097411,-0.416466, 0.075186,-0.463541 },
{  0.208191,-0.295704, 0.049512,-0.382158, 0.072149, 0.187547,-0.440613, 0.126406,-0.428072, 0.496233,-0.037666,-0.453715,-0.538535, 0.238543,-0.855363,-0.340692,-0.138209 },
{  0.833307,-0.386819, 0.076835,-0.496991, 0.541043,-0.409259,-0.158463,-0.104847, 0.336935,-0.323590, 0.233353,-0.185794,-0.473744,-0.642914,-0.222137, 0.037842,-0.733056 },
{  0.412360,-0.388089,-0.126896,-0.187061, 0.220804, 0.027855,-0.306205,-0.009230,-0.138758, 0.354466, 0.432872, 0.350325,-0.264302, 0.320194,-0.378596, 0.008075,-0.416725 },
{ -0.235829, 0.212778, 0.311359,-0.773900,-1.114307, 1.289023,-0.370446,-0.855490,-0.307593,-0.383002, 0.508173,-0.336553,-1.004212,-0.565306,-0.935519,-0.656930, 2.186084 },
{  1.079223,-0.341324,-0.497928, 0.678790, 1.088938, 0.505797, 1.557402, 0.078280,-0.299738, 0.871413,-0.329088, 1.895734,-0.785482, 1.152082,-1.048376, 1.565468,-1.371850 },
{ -0.288824,-0.419145,-0.152938,-0.084120, 0.505475,-0.244922, 0.433470,-0.366537, 0.341296,-0.471731,-0.371332,-0.131464,-0.133145,-0.353261,-0.273516,-0.035949,-0.171409 },
{ -0.509058, 0.141947,-0.148900, 0.484579,-0.305297, 0.167928, 0.151546, 1.366509,-0.618748, 1.333630, 0.283760, 0.196043,-0.799434,-0.049077,-0.888869,-0.226980,-0.959439 },
{  0.732656,-0.065899,-0.287768,-0.447145, 0.004897, 0.390873,-0.076184, 0.188104,-0.060508,-0.079252,-0.240235,-0.116149,-0.552348, 0.034669,-0.209985,-0.039063,-0.398936 },
{ -0.564401,-1.027148,-0.007026, 0.063725, 1.090305, 0.061625, 0.803589,-0.751013,-0.248919,-0.582876,-0.297927, 0.319461,-0.330912, 0.263864,-0.607158, 1.041071,-0.283908 },
};
static double savedWeightTensor_4[layer4_neuronNum][layer3_neuronNum + 1] =
{
{ -1.615681,-1.876205, 0.364524, 0.931561,-1.336012, 0.551689,-2.181580, 2.298886, 0.623580, 1.996476, 0.824607, 1.793902, 0.003646 },
};

// 把保存的权重复制到这上面

static double weightTensor_1[layer1_neuronNum][inputLength + 1] = { 0 }; // 第一层权重矩阵
static double weightTensor_2[layer2_neuronNum][layer1_neuronNum + 1] = { 0 }; // 第二层权重矩阵
static double weightTensor_3[layer3_neuronNum][layer2_neuronNum + 1] = { 0 }; // 第三层权重矩阵
static double weightTensor_4[layer4_neuronNum][layer3_neuronNum + 1] = { 0 }; // 输出层权重矩阵

static double layer1_outputTensor[layer1_neuronNum] = { 0 };
static double layer2_outputTensor[layer2_neuronNum] = { 0 };
static double layer3_outputTensor[layer3_neuronNum] = { 0 };
static double layer4_outputTensor[layer4_neuronNum] = { 0 };

static void UpdateWeights(
    double* inputTensor,
    double* layer1_outputTensor,
    double* layer2_outputTensor,
    double* layer3_outputTensor,
    double* layer4_outputTensor,
    double* labelTensor,
    double weightTensor_1[][inputLength + 1],
    double weightTensor_2[][layer1_neuronNum + 1],
    double weightTensor_3[][layer2_neuronNum + 1],
    double weightTensor_4[][layer3_neuronNum + 1],
    double learningRate) {

    // 计算第四层梯度
    double layer4_error = LossDerivative(layer4_outputTensor[0], labelTensor[0]);
    layer4_error *= TanhDerivative(layer4_outputTensor[0]);
    // 计算第三层梯度
    double layer3_error[layer3_neuronNum];
    for (int j = 0; j < layer3_neuronNum; j++) {
        layer3_error[j] = layer4_error * weightTensor_4[0][j];
        layer3_error[j] *= ReLUDerivative(layer3_outputTensor[j]);
    }
    // 计算第二层梯度
    double layer2_error[layer2_neuronNum];
    for (int j = 0; j < layer2_neuronNum; j++) {
        layer2_error[j] = 0;
        for (int k = 0; k < layer3_neuronNum; k++) {
            layer2_error[j] += layer3_error[k] * weightTensor_3[k][j];
        }
        layer2_error[j] *= ReLUDerivative(layer2_outputTensor[j]);
    }
    // 计算第一层梯度
    double layer1_error[layer1_neuronNum];
    for (int j = 0; j < layer1_neuronNum; j++) {
        layer1_error[j] = 0;
        for (int k = 0; k < layer2_neuronNum; k++) {
            layer1_error[j] += layer2_error[k] * weightTensor_2[k][j];
        }
        layer1_error[j] *= ReLUDerivative(layer1_outputTensor[j]);
    }

    // 更新第四层权重和偏置
    for (int j = 0; j < layer3_neuronNum; j++) {
        weightTensor_4[0][j] -= learningRate * layer4_error * layer3_outputTensor[j];
    }
    weightTensor_4[0][layer3_neuronNum] -= learningRate * layer4_error;
    // 更新第三层权重和偏置
    for (int j = 0; j < layer3_neuronNum; j++) {
        for (int k = 0; k < layer2_neuronNum; k++) {
            weightTensor_3[j][k] -= learningRate * layer3_error[j] * layer2_outputTensor[k];
        }
        weightTensor_3[j][layer2_neuronNum] -= learningRate * layer3_error[j];
    }
    // 更新第二层权重和偏置
    for (int j = 0; j < layer2_neuronNum; j++) {
        for (int k = 0; k < layer1_neuronNum; k++) {
            weightTensor_2[j][k] -= learningRate * layer2_error[j] * layer1_outputTensor[k];
        }
        weightTensor_2[j][layer1_neuronNum] -= learningRate * layer2_error[j];
    }
    // 更新第一层权重和偏置
    for (int j = 0; j < layer1_neuronNum; j++) {
        for (int k = 0; k < inputLength; k++) {
            weightTensor_1[j][k] -= learningRate * layer1_error[j] * inputTensor[k];
        }
        weightTensor_1[j][inputLength] -= learningRate * layer1_error[j];
    }
}

static double Forward(
    double* inputTensor,
    double weightTensor_1[][inputLength + 1],
    double weightTensor_2[][layer1_neuronNum + 1],
    double weightTensor_3[][layer2_neuronNum + 1],
    double weightTensor_4[][layer3_neuronNum + 1],
    double* layer1_outputTensor,
    double* layer2_outputTensor,
    double* layer3_outputTensor,
    double* layer4_outputTensor) {
    // 第一层有 layer1_neuronNum 个神经元
    for (int i = 0; i < layer1_neuronNum; i++) {
        layer1_outputTensor[i] = Linear(inputTensor, inputLength, weightTensor_1[i]);
        layer1_outputTensor[i] = ReLU(layer1_outputTensor[i]);
    }
    // 第二层有 layer2_neuronNum 个神经元
    for (int i = 0; i < layer2_neuronNum; i++) {
        layer2_outputTensor[i] = Linear(layer1_outputTensor, layer1_neuronNum, weightTensor_2[i]);
        layer2_outputTensor[i] = ReLU(layer2_outputTensor[i]);
    }
    // 第三层有 layer3_neuronNum 个神经元
    for (int i = 0; i < layer3_neuronNum; i++) {
        layer3_outputTensor[i] = Linear(layer2_outputTensor, layer2_neuronNum, weightTensor_3[i]);
        layer3_outputTensor[i] = ReLU(layer3_outputTensor[i]);
    }
    // 输出层只有一个神经元
    layer4_outputTensor[0] = Linear(layer3_outputTensor, layer3_neuronNum, weightTensor_4[0]);
    layer4_outputTensor[0] = Tanh(layer4_outputTensor[0]);

    return layer4_outputTensor[0];
}

// 以下是对外函数

void UpdateWeights_4Layers_NN(double* inputTensor, double* labelTensor, double learningRate) {
    UpdateWeights(
        inputTensor,
        layer1_outputTensor,
        layer2_outputTensor,
        layer3_outputTensor,
        layer4_outputTensor,
        labelTensor,
        weightTensor_1,
        weightTensor_2,
        weightTensor_3,
        weightTensor_4,
        learningRate);
}

double Forward_4Layers_NN(double* inputTensor) {
    return Forward(
        inputTensor, 
        weightTensor_1, 
        weightTensor_2, 
        weightTensor_3, 
        weightTensor_4,
        layer1_outputTensor, 
        layer2_outputTensor, 
        layer3_outputTensor, 
        layer4_outputTensor);
}

void Randomized_4Layers_NN_Weight(int seed) {
    srand(seed);
    // ReLU 神经元用 He 初始化权重，Sigmoid 神经元用 Xavier 初始化权重
    XavierInitialize(&weightTensor_1[0][0], layer1_neuronNum, inputLength + 1);
    XavierInitialize(&weightTensor_2[0][0], layer2_neuronNum, layer1_neuronNum + 1);
    XavierInitialize(&weightTensor_3[0][0], layer3_neuronNum, layer2_neuronNum + 1);
    HeInitialize(&weightTensor_4[0][0], layer4_neuronNum, layer3_neuronNum + 1);
    printf("Weight Randomized.\n");
}

void Print_4Layers_NN_Weight() {
    printf("static double savedWeightTensor_1[layer1_neuronNum][inputLength + 1] = \n");
    Print_Tensor((double*)weightTensor_1, layer1_neuronNum, inputLength + 1);
    printf("static double savedWeightTensor_2[layer2_neuronNum][layer1_neuronNum + 1] = \n");
    Print_Tensor((double*)weightTensor_2, layer2_neuronNum, layer1_neuronNum + 1);
    printf("static double savedWeightTensor_3[layer3_neuronNum][layer2_neuronNum + 1] = \n");
    Print_Tensor((double*)weightTensor_3, layer3_neuronNum, layer2_neuronNum + 1);
    printf("static double savedWeightTensor_4[layer4_neuronNum][layer3_neuronNum + 1] = \n");
    Print_Tensor((double*)weightTensor_4, layer4_neuronNum, layer3_neuronNum + 1);
}

void Load_4layers_NN_Weight() {
    for (int i = 0; i < layer1_neuronNum; i++) {
        for (int j = 0; j < inputLength + 1; j++) {
            weightTensor_1[i][j] = savedWeightTensor_1[i][j];
        }
    }
    for (int i = 0; i < layer2_neuronNum; i++) {
        for (int j = 0; j < layer1_neuronNum + 1; j++) {
            weightTensor_2[i][j] = savedWeightTensor_2[i][j];
        }
    }
    for (int i = 0; i < layer3_neuronNum; i++) {
        for (int j = 0; j < layer2_neuronNum + 1; j++) {
            weightTensor_3[i][j] = savedWeightTensor_3[i][j];
        }
    }
    for (int i = 0; i < layer4_neuronNum; i++) {
        for (int j = 0; j < layer3_neuronNum + 1; j++) {
            weightTensor_4[i][j] = savedWeightTensor_4[i][j];
        }
    }
    printf("Loaded 4 Layers Dense NN Weight. (%d -> %d -> %d -> %d)\n",
        layer1_neuronNum, layer2_neuronNum, layer3_neuronNum, layer4_neuronNum);
}